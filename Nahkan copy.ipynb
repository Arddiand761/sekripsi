{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 7265,
     "status": "ok",
     "timestamp": 1759364299410,
     "user": {
      "displayName": "Arddian",
      "userId": "16616596264076823747"
     },
     "user_tz": -420
    },
    "id": "6iAOOx4N5pTE"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "random.seed(RANDOM_STATE)\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, precision_recall_curve, auc\n",
    "\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30,
     "status": "ok",
     "timestamp": 1759364299443,
     "user": {
      "displayName": "Arddian",
      "userId": "16616596264076823747"
     },
     "user_tz": -420
    },
    "id": "mWMqigZX5w05",
    "outputId": "9e688bdc-91c3-42e6-9c5b-b28a1473ec9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ukuran dataset: (4240, 16)\n",
      "\n",
      "Jumlah missing value per kolom:\n",
      " male                 0\n",
      "age                  0\n",
      "education          105\n",
      "currentSmoker        0\n",
      "cigsPerDay          29\n",
      "BPMeds              53\n",
      "prevalentStroke      0\n",
      "prevalentHyp         0\n",
      "diabetes             0\n",
      "totChol             50\n",
      "sysBP                0\n",
      "diaBP                0\n",
      "BMI                 19\n",
      "heartRate            1\n",
      "glucose            388\n",
      "TenYearCHD           0\n",
      "dtype: int64\n",
      "\n",
      "Info dataset:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4240 entries, 0 to 4239\n",
      "Data columns (total 16 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   male             4240 non-null   int64  \n",
      " 1   age              4240 non-null   int64  \n",
      " 2   education        4135 non-null   float64\n",
      " 3   currentSmoker    4240 non-null   int64  \n",
      " 4   cigsPerDay       4211 non-null   float64\n",
      " 5   BPMeds           4187 non-null   float64\n",
      " 6   prevalentStroke  4240 non-null   int64  \n",
      " 7   prevalentHyp     4240 non-null   int64  \n",
      " 8   diabetes         4240 non-null   int64  \n",
      " 9   totChol          4190 non-null   float64\n",
      " 10  sysBP            4240 non-null   float64\n",
      " 11  diaBP            4240 non-null   float64\n",
      " 12  BMI              4221 non-null   float64\n",
      " 13  heartRate        4239 non-null   float64\n",
      " 14  glucose          3852 non-null   float64\n",
      " 15  TenYearCHD       4240 non-null   int64  \n",
      "dtypes: float64(9), int64(7)\n",
      "memory usage: 530.1 KB\n",
      "None\n",
      "\n",
      "Distribusi label target (TenYearCHD):\n",
      "TenYearCHD\n",
      "0    0.848113\n",
      "1    0.151887\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"framingham.csv\")\n",
    "\n",
    "\n",
    "print(\"Ukuran dataset:\", df.shape)\n",
    "print(\"\\nJumlah missing value per kolom:\\n\", df.isnull().sum())\n",
    "\n",
    "\n",
    "print(\"\\nInfo dataset:\")\n",
    "print(df.info())\n",
    "\n",
    "if \"TenYearCHD\" in df.columns:\n",
    "    print(\"\\nDistribusi label target (TenYearCHD):\")\n",
    "    print(df[\"TenYearCHD\"].value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1759364299492,
     "user": {
      "displayName": "Arddian",
      "userId": "16616596264076823747"
     },
     "user_tz": -420
    },
    "id": "RwP6fWev6D7T",
    "outputId": "18eb34ef-ebc5-4f26-e70e-d1c8d0567cc9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah missing value setelah imputasi:\n",
      " 0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns\n",
    "cat_cols = df.select_dtypes(exclude=[np.number]).columns\n",
    "\n",
    "imputer_num = SimpleImputer(strategy=\"median\")\n",
    "df[num_cols] = imputer_num.fit_transform(df[num_cols])\n",
    "\n",
    "if len(cat_cols) > 0:\n",
    "    imputer_cat = SimpleImputer(strategy=\"most_frequent\")\n",
    "    df[cat_cols] = imputer_cat.fit_transform(df[cat_cols])\n",
    "\n",
    "print(\"Jumlah missing value setelah imputasi:\\n\", df.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1759364504729,
     "user": {
      "displayName": "Arddian",
      "userId": "16616596264076823747"
     },
     "user_tz": -420
    },
    "id": "6X3QOyuP5x6_",
    "outputId": "db7106ce-a202-4d8e-ad79-8c98fa73780e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribusi kelas di train: [2517  451]\n",
      "Distribusi kelas di test : [1079  193]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop(\"TenYearCHD\", axis=1)\n",
    "y = df[\"TenYearCHD\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, shuffle=True, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Distribusi kelas di train:\", np.bincount(y_train))\n",
    "print(\"Distribusi kelas di test :\", np.bincount(y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1759364506159,
     "user": {
      "displayName": "Arddian",
      "userId": "16616596264076823747"
     },
     "user_tz": -420
    },
    "id": "HV4DcOtz53Gm",
    "outputId": "b8e3e8c3-b793-4823-db7b-da47a993edeb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribusi y_train: Counter({0.0: 2517, 1.0: 451})\n",
      "Distribusi y_test: Counter({0.0: 1079, 1.0: 193})\n",
      "\n",
      "Baseline (most_frequent) Accuracy: 0.8482704402515723\n",
      "Baseline (most_frequent) Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0      0.848     1.000     0.918      1079\n",
      "         1.0      0.000     0.000     0.000       193\n",
      "\n",
      "    accuracy                          0.848      1272\n",
      "   macro avg      0.424     0.500     0.459      1272\n",
      "weighted avg      0.720     0.848     0.779      1272\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell A: cek distribusi label dan baseline majority\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix\n",
    "\n",
    "print(\"Distribusi y_train:\", Counter(y_train))\n",
    "print(\"Distribusi y_test:\", Counter(y_test))\n",
    "\n",
    "# Baseline majority\n",
    "dummy = DummyClassifier(strategy='most_frequent')\n",
    "dummy.fit(X_train, y_train)\n",
    "y_dummy = dummy.predict(X_test)\n",
    "print(\"\\nBaseline (most_frequent) Accuracy:\", accuracy_score(y_test, y_dummy))\n",
    "print(\"Baseline (most_frequent) Classification Report:\\n\", classification_report(y_test, y_dummy, digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 115,
     "status": "ok",
     "timestamp": 1759364508588,
     "user": {
      "displayName": "Arddian",
      "userId": "16616596264076823747"
     },
     "user_tz": -420
    },
    "id": "P6cClY2056Kr",
    "outputId": "9b796e74-3871-4055-8e09-774f09f76f52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2968, 15) -> (2968, 15)\n",
      "(1272, 15) -> (1272, 15)\n"
     ]
    }
   ],
   "source": [
    "# Cell B: scaler yang benar (StandardScaler contoh)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)   # FIT hanya di train\n",
    "X_test_scaled  = scaler.transform(X_test)        # TRANSFORM di test\n",
    "\n",
    "# Cek shape\n",
    "print(X_train.shape, '->', X_train_scaled.shape)\n",
    "print(X_test.shape, '->', X_test_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Bn57X9O657Tu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM (no class_weight) Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0      0.848     1.000     0.918      1079\n",
      "         1.0      0.000     0.000     0.000       193\n",
      "\n",
      "    accuracy                          0.848      1272\n",
      "   macro avg      0.424     0.500     0.459      1272\n",
      "weighted avg      0.720     0.848     0.779      1272\n",
      "\n",
      "Accuracy: 0.8482704402515723\n",
      "F1-score (pos=1): 0.0\n",
      "Confusion Matrix:\n",
      " [[1079    0]\n",
      " [ 193    0]]\n"
     ]
    }
   ],
   "source": [
    "# Cell C: SVM tanpa class_weight (mungkin mirip dengan baseline dosen)\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_plain = SVC(kernel='linear', C=1.0, probability=True, random_state=42)  # gamma dihapus karena linear\n",
    "svm_plain.fit(X_train_scaled, y_train)\n",
    "y_pred_plain = svm_plain.predict(X_test_scaled)\n",
    "\n",
    "print(\"SVM (no class_weight) Classification Report:\\n\", classification_report(y_test, y_pred_plain, digits=3))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_plain))\n",
    "print(\"F1-score (pos=1):\", f1_score(y_test, y_pred_plain, pos_label=1))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_plain))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "jyoYLa7058Ul"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah setelah SMOTE: (3775, 15) [2517 1258]\n",
      "\n",
      "=== HASIL SVM + SMOTE ===\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0      0.868     0.930     0.898      1079\n",
      "         1.0      0.350     0.212     0.265       193\n",
      "\n",
      "    accuracy                          0.821      1272\n",
      "   macro avg      0.609     0.571     0.581      1272\n",
      "weighted avg      0.790     0.821     0.802      1272\n",
      "\n",
      "F1-Score: 0.2645161290322581\n",
      "\n",
      "=== PERBANDINGAN ===\n",
      "F1-Score Baseline SVM: 0.0\n",
      "F1-Score SVM + SMOTE: 0.2645161290322581\n",
      "Improvement: 0.2645161290322581\n",
      "\n",
      "=== HASIL SVM + SMOTE ===\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0      0.868     0.930     0.898      1079\n",
      "         1.0      0.350     0.212     0.265       193\n",
      "\n",
      "    accuracy                          0.821      1272\n",
      "   macro avg      0.609     0.571     0.581      1272\n",
      "weighted avg      0.790     0.821     0.802      1272\n",
      "\n",
      "F1-Score: 0.2645161290322581\n",
      "\n",
      "=== PERBANDINGAN ===\n",
      "F1-Score Baseline SVM: 0.0\n",
      "F1-Score SVM + SMOTE: 0.2645161290322581\n",
      "Improvement: 0.2645161290322581\n"
     ]
    }
   ],
   "source": [
    "# SMOTE + SVM\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "# Terapkan SMOTE pada data training\n",
    "smote = SMOTE(sampling_strategy=0.5, random_state=42, k_neighbors=3)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Jumlah setelah SMOTE:\", X_resampled.shape, np.bincount(y_resampled))\n",
    "\n",
    "# SVM dengan parameter yang lebih optimal\n",
    "svm_smote = SVC(kernel='linear', C=0.5, random_state=42)  # Lebih simpel dan cepat\n",
    "svm_smote.fit(X_resampled, y_resampled)\n",
    "y_pred_smote = svm_smote.predict(X_test_scaled)\n",
    "\n",
    "print(\"\\n=== HASIL SVM + SMOTE ===\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_smote, digits=3))\n",
    "print(\"F1-Score:\", f1_score(y_test, y_pred_smote))\n",
    "\n",
    "# Bandingkan dengan baseline SVM\n",
    "print(\"\\n=== PERBANDINGAN ===\")\n",
    "print(\"F1-Score Baseline SVM:\", f1_score(y_test, y_pred_plain))\n",
    "print(\"F1-Score SVM + SMOTE:\", f1_score(y_test, y_pred_smote))\n",
    "print(\"Improvement:\", f1_score(y_test, y_pred_smote) - f1_score(y_test, y_pred_plain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "\n",
    "class SMOTE_IPF(BaseEstimator):\n",
    "    \"\"\"\n",
    "    SMOTE-IPF: SMOTE dengan Iterative Partitioning Filter\n",
    "    Optimized version untuk skripsi\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, smote_k=5, ipf_k=5, max_iter=10, sampling_strategy=0.5,\n",
    "                 random_state=None, verbose=False, remove_only_synthetic=True):\n",
    "        # Parameter validation\n",
    "        if smote_k < 1 or ipf_k < 1:\n",
    "            raise ValueError(\"k values must be >= 1\")\n",
    "        if max_iter < 1:\n",
    "            raise ValueError(\"max_iter must be >= 1\")\n",
    "            \n",
    "        self.smote_k = smote_k\n",
    "        self.ipf_k = min(ipf_k, smote_k)  # IPF k tidak boleh > SMOTE k\n",
    "        self.max_iter = max_iter\n",
    "        self.sampling_strategy = sampling_strategy\n",
    "        self.random_state = random_state\n",
    "        self.verbose = verbose\n",
    "        self.remove_only_synthetic = remove_only_synthetic\n",
    "        \n",
    "    def fit_resample(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit dan resample data menggunakan SMOTE-IPF\n",
    "        \"\"\"\n",
    "        X = np.asarray(X, dtype=np.float64)\n",
    "        y = np.asarray(y).astype(int).ravel()\n",
    "        \n",
    "        if len(np.unique(y)) < 2:\n",
    "            raise ValueError(\"Need at least 2 classes\")\n",
    "            \n",
    "        n_orig = X.shape[0]\n",
    "        \n",
    "        # Step 1: Apply SMOTE\n",
    "        smote = SMOTE(\n",
    "            k_neighbors=self.smote_k,\n",
    "            sampling_strategy=self.sampling_strategy,\n",
    "            random_state=self.random_state\n",
    "        )\n",
    "        \n",
    "        X_res, y_res = smote.fit_resample(X, y)\n",
    "        \n",
    "        # Step 2: Track synthetic samples\n",
    "        n_after_smote = X_res.shape[0]\n",
    "        n_synthetic = n_after_smote - n_orig\n",
    "        \n",
    "        # Boolean mask untuk synthetic samples\n",
    "        is_synthetic = np.zeros(n_after_smote, dtype=bool)\n",
    "        if n_synthetic > 0:\n",
    "            is_synthetic[n_orig:] = True\n",
    "            \n",
    "        if self.verbose:\n",
    "            unique, counts = np.unique(y_res, return_counts=True)\n",
    "            print(f\"[SMOTE] Total: {n_after_smote}, Synthetic: {n_synthetic}\")\n",
    "            print(f\"[SMOTE] Class distribution: {dict(zip(unique, counts))}\")\n",
    "        \n",
    "        # Step 3: Iterative Partitioning Filter\n",
    "        X_current = X_res.copy()\n",
    "        y_current = y_res.copy()\n",
    "        synthetic_current = is_synthetic.copy()\n",
    "        \n",
    "        for iteration in range(self.max_iter):\n",
    "            if len(X_current) == 0:\n",
    "                break\n",
    "                \n",
    "            # Train KNN classifier\n",
    "            n_neighbors = min(self.ipf_k, len(X_current) - 1)\n",
    "            if n_neighbors < 1:\n",
    "                break\n",
    "                \n",
    "            clf = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "            clf.fit(X_current, y_current)\n",
    "            y_pred = clf.predict(X_current)\n",
    "            \n",
    "            # Find misclassified samples\n",
    "            misclassified = (y_pred != y_current)\n",
    "            n_misclassified = misclassified.sum()\n",
    "            \n",
    "            if n_misclassified == 0:\n",
    "                if self.verbose:\n",
    "                    print(f\"[IPF] Iter {iteration+1}: No misclassified -> STOP\")\n",
    "                break\n",
    "            \n",
    "            # Determine which samples to remove\n",
    "            if self.remove_only_synthetic:\n",
    "                # Only remove synthetic misclassified samples\n",
    "                to_remove = misclassified & synthetic_current\n",
    "                n_removed = to_remove.sum()\n",
    "                \n",
    "                if n_removed == 0:\n",
    "                    if self.verbose:\n",
    "                        print(f\"[IPF] Iter {iteration+1}: Only original misclassified -> STOP\")\n",
    "                    break\n",
    "            else:\n",
    "                # Remove all misclassified samples\n",
    "                to_remove = misclassified\n",
    "                n_removed = to_remove.sum()\n",
    "            \n",
    "            # Update arrays\n",
    "            keep_mask = ~to_remove\n",
    "            X_current = X_current[keep_mask]\n",
    "            y_current = y_current[keep_mask]\n",
    "            synthetic_current = synthetic_current[keep_mask]\n",
    "            \n",
    "            if self.verbose:\n",
    "                unique, counts = np.unique(y_current, return_counts=True)\n",
    "                print(f\"[IPF] Iter {iteration+1}: Removed {n_removed}/{n_misclassified}, \"\n",
    "                      f\"Remaining: {len(X_current)}, \"\n",
    "                      f\"Distribution: {dict(zip(unique, counts))}\")\n",
    "        \n",
    "        if self.verbose:\n",
    "            unique, counts = np.unique(y_current, return_counts=True)\n",
    "            synthetic_remaining = synthetic_current.sum()\n",
    "            print(f\"[FINAL] Total: {len(X_current)}, Synthetic remaining: {synthetic_remaining}\")\n",
    "            print(f\"[FINAL] Class distribution: {dict(zip(unique, counts))}\")\n",
    "        \n",
    "        return X_current, y_current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TESTING SMOTE-IPF ===\n",
      "[SMOTE] Total: 3775, Synthetic: 807\n",
      "[SMOTE] Class distribution: {np.int64(0): np.int64(2517), np.int64(1): np.int64(1258)}\n",
      "[IPF] Iter 1: Removed 1/352, Remaining: 3774, Distribution: {np.int64(0): np.int64(2517), np.int64(1): np.int64(1257)}\n",
      "[IPF] Iter 2: Only original misclassified -> STOP\n",
      "[FINAL] Total: 3774, Synthetic remaining: 806\n",
      "[FINAL] Class distribution: {np.int64(0): np.int64(2517), np.int64(1): np.int64(1257)}\n",
      "\n",
      "Distribusi akhir SMOTE-IPF: [2517 1257]\n",
      "\n",
      "=== HASIL SVM + SMOTE-IPF ===\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0      0.870     0.927     0.897      1079\n",
      "         1.0      0.352     0.223     0.273       193\n",
      "\n",
      "    accuracy                          0.820      1272\n",
      "   macro avg      0.611     0.575     0.585      1272\n",
      "weighted avg      0.791     0.820     0.803      1272\n",
      "\n",
      "F1-Score: 0.273015873015873\n",
      "\n",
      "=== PERBANDINGAN LENGKAP ===\n",
      "F1-Score Baseline SVM  : 0.0\n",
      "F1-Score SVM + SMOTE   : 0.2645161290322581\n",
      "F1-Score SVM + SMOTE-IPF: 0.273015873015873\n",
      "Improvement SMOTE-IPF vs SMOTE: 0.008499743983614916\n"
     ]
    }
   ],
   "source": [
    "# Test SMOTE-IPF yang sudah dioptimalkan\n",
    "print(\"=== TESTING SMOTE-IPF ===\")\n",
    "\n",
    "# Inisialisasi SMOTE-IPF dengan parameter optimal\n",
    "smote_ipf = SMOTE_IPF(\n",
    "    smote_k=5, \n",
    "    ipf_k=3, \n",
    "    max_iter=10, \n",
    "    sampling_strategy=0.5,\n",
    "    random_state=42, \n",
    "    verbose=True,\n",
    "    remove_only_synthetic=True\n",
    ")\n",
    "\n",
    "# Apply SMOTE-IPF\n",
    "X_ipf, y_ipf = smote_ipf.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"\\nDistribusi akhir SMOTE-IPF: {np.bincount(y_ipf)}\")\n",
    "\n",
    "# Train SVM dengan data SMOTE-IPF\n",
    "svm_ipf = SVC(kernel='linear', C=1.0, random_state=42)\n",
    "svm_ipf.fit(X_ipf, y_ipf)\n",
    "y_pred_ipf = svm_ipf.predict(X_test_scaled)\n",
    "\n",
    "print(\"\\n=== HASIL SVM + SMOTE-IPF ===\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_ipf, digits=3))\n",
    "print(\"F1-Score:\", f1_score(y_test, y_pred_ipf))\n",
    "\n",
    "# Perbandingan semua metode\n",
    "print(\"\\n=== PERBANDINGAN LENGKAP ===\")\n",
    "print(\"F1-Score Baseline SVM  :\", f1_score(y_test, y_pred_plain))\n",
    "print(\"F1-Score SVM + SMOTE   :\", f1_score(y_test, y_pred_smote))\n",
    "print(\"F1-Score SVM + SMOTE-IPF:\", f1_score(y_test, y_pred_ipf))\n",
    "print(\"Improvement SMOTE-IPF vs SMOTE:\", f1_score(y_test, y_pred_ipf) - f1_score(y_test, y_pred_smote))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EKSPERIMEN MENGURANGI BIAS ===\n",
      "[SMOTE] Total: 4530, Synthetic: 1562\n",
      "[SMOTE] Class distribution: {np.int64(0): np.int64(2517), np.int64(1): np.int64(2013)}\n",
      "[IPF] Iter 1: Removed 3/407, Remaining: 4527, Distribution: {np.int64(0): np.int64(2517), np.int64(1): np.int64(2010)}\n",
      "[IPF] Iter 2: Only original misclassified -> STOP\n",
      "[FINAL] Total: 4527, Synthetic remaining: 1559\n",
      "[FINAL] Class distribution: {np.int64(0): np.int64(2517), np.int64(1): np.int64(2010)}\n",
      "[IPF] Iter 2: Only original misclassified -> STOP\n",
      "[FINAL] Total: 4527, Synthetic remaining: 1559\n",
      "[FINAL] Class distribution: {np.int64(0): np.int64(2517), np.int64(1): np.int64(2010)}\n",
      "\n",
      "=== HASIL SVM + SMOTE-IPF + CLASS_WEIGHT ===\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0      0.909     0.641     0.752      1079\n",
      "         1.0      0.243     0.642     0.352       193\n",
      "\n",
      "    accuracy                          0.642      1272\n",
      "   macro avg      0.576     0.642     0.552      1272\n",
      "weighted avg      0.808     0.642     0.691      1272\n",
      "\n",
      "F1-Score: 0.3522727272727273\n",
      "\n",
      "=== HASIL SVM + SMOTE-IPF + CLASS_WEIGHT ===\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0      0.909     0.641     0.752      1079\n",
      "         1.0      0.243     0.642     0.352       193\n",
      "\n",
      "    accuracy                          0.642      1272\n",
      "   macro avg      0.576     0.642     0.552      1272\n",
      "weighted avg      0.808     0.642     0.691      1272\n",
      "\n",
      "F1-Score: 0.3522727272727273\n",
      "\n",
      "=== THRESHOLD TUNING ===\n",
      "Threshold 0.3: F1=0.359, Recall=0.668, Precision=0.245\n",
      "Threshold 0.4: F1=0.349, Recall=0.456, Precision=0.282\n",
      "Threshold 0.5: F1=0.316, Recall=0.316, Precision=0.316\n",
      "Threshold 0.6: F1=0.251, Recall=0.187, Precision=0.383\n",
      "Threshold 0.7: F1=0.157, Recall=0.098, Precision=0.388\n",
      "\n",
      "=== PERBANDINGAN BIAS (LENGKAP) ===\n",
      "Method                    | F1-Score | Recall-0 | Recall-1 | Bias\n",
      "--------------------------------------------------------------------\n",
      "Baseline SVM              | 0.000    | 1.000    | 0.000    | Ekstrem\n",
      "SVM + SMOTE               | 0.265    | 0.930    | 0.212    | Tinggi\n",
      "SVM + SMOTE-IPF           | 0.273    | 0.927    | 0.223    | Tinggi\n",
      "SVM + SMOTE-IPF + Balanced| 0.352    | 0.641    | 0.642    | Rendah\n",
      "\n",
      "=== ANALISIS BIAS REDUCTION ===\n",
      "Baseline: Recall gap = 1.000 (Ekstrem bias)\n",
      "SMOTE: Recall gap = 0.717 (Tinggi bias)\n",
      "SMOTE-IPF: Recall gap = 0.704 (Tinggi bias)\n",
      "Balanced: Recall gap = 0.001 (Rendah bias)\n",
      "\n",
      "ðŸŽ¯ KESIMPULAN:\n",
      "â€¢ Baseline: F1=0.000, Miss 100.0% kasus CHD\n",
      "â€¢ SMOTE: F1=0.265, Miss 78.8% kasus CHD\n",
      "â€¢ SMOTE-IPF: F1=0.273, Miss 77.7% kasus CHD\n",
      "â€¢ Balanced: F1=0.352, Miss 35.8% kasus CHD âœ…\n",
      "\n",
      "ðŸ“Š IMPROVEMENT:\n",
      "â€¢ SMOTE-IPF vs SMOTE: +0.008 F1-score\n",
      "â€¢ Balanced vs SMOTE-IPF: +0.079 F1-score\n",
      "â€¢ Total improvement: +0.352 F1-score\n",
      "\n",
      "=== THRESHOLD TUNING ===\n",
      "Threshold 0.3: F1=0.359, Recall=0.668, Precision=0.245\n",
      "Threshold 0.4: F1=0.349, Recall=0.456, Precision=0.282\n",
      "Threshold 0.5: F1=0.316, Recall=0.316, Precision=0.316\n",
      "Threshold 0.6: F1=0.251, Recall=0.187, Precision=0.383\n",
      "Threshold 0.7: F1=0.157, Recall=0.098, Precision=0.388\n",
      "\n",
      "=== PERBANDINGAN BIAS (LENGKAP) ===\n",
      "Method                    | F1-Score | Recall-0 | Recall-1 | Bias\n",
      "--------------------------------------------------------------------\n",
      "Baseline SVM              | 0.000    | 1.000    | 0.000    | Ekstrem\n",
      "SVM + SMOTE               | 0.265    | 0.930    | 0.212    | Tinggi\n",
      "SVM + SMOTE-IPF           | 0.273    | 0.927    | 0.223    | Tinggi\n",
      "SVM + SMOTE-IPF + Balanced| 0.352    | 0.641    | 0.642    | Rendah\n",
      "\n",
      "=== ANALISIS BIAS REDUCTION ===\n",
      "Baseline: Recall gap = 1.000 (Ekstrem bias)\n",
      "SMOTE: Recall gap = 0.717 (Tinggi bias)\n",
      "SMOTE-IPF: Recall gap = 0.704 (Tinggi bias)\n",
      "Balanced: Recall gap = 0.001 (Rendah bias)\n",
      "\n",
      "ðŸŽ¯ KESIMPULAN:\n",
      "â€¢ Baseline: F1=0.000, Miss 100.0% kasus CHD\n",
      "â€¢ SMOTE: F1=0.265, Miss 78.8% kasus CHD\n",
      "â€¢ SMOTE-IPF: F1=0.273, Miss 77.7% kasus CHD\n",
      "â€¢ Balanced: F1=0.352, Miss 35.8% kasus CHD âœ…\n",
      "\n",
      "ðŸ“Š IMPROVEMENT:\n",
      "â€¢ SMOTE-IPF vs SMOTE: +0.008 F1-score\n",
      "â€¢ Balanced vs SMOTE-IPF: +0.079 F1-score\n",
      "â€¢ Total improvement: +0.352 F1-score\n"
     ]
    }
   ],
   "source": [
    "print(\"=== EKSPERIMEN MENGURANGI BIAS ===\")\n",
    "\n",
    "# 1. Coba sampling_strategy lebih agresif (lebih balanced)\n",
    "smote_ipf_balanced = SMOTE_IPF(\n",
    "    smote_k=5, \n",
    "    ipf_k=3, \n",
    "    max_iter=10, \n",
    "    sampling_strategy=0.8,  # Lebih agresif\n",
    "    random_state=42, \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "X_ipf_bal, y_ipf_bal = smote_ipf_balanced.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# 2. SVM dengan class_weight balanced\n",
    "svm_balanced = SVC(kernel='linear', C=1.0, class_weight='balanced', random_state=42)\n",
    "svm_balanced.fit(X_ipf_bal, y_ipf_bal)\n",
    "y_pred_balanced = svm_balanced.predict(X_test_scaled)\n",
    "\n",
    "print(\"\\n=== HASIL SVM + SMOTE-IPF + CLASS_WEIGHT ===\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_balanced, digits=3))\n",
    "print(\"F1-Score:\", f1_score(y_test, y_pred_balanced))\n",
    "\n",
    "# 3. Coba decision threshold tuning dengan probability\n",
    "svm_prob = SVC(kernel='linear', C=1.0, probability=True, random_state=42)\n",
    "svm_prob.fit(X_ipf, y_ipf)\n",
    "y_prob = svm_prob.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Tuning threshold untuk balance precision-recall\n",
    "thresholds = [0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "print(\"\\n=== THRESHOLD TUNING ===\")\n",
    "for thresh in thresholds:\n",
    "    y_pred_thresh = (y_prob >= thresh).astype(int)\n",
    "    f1 = f1_score(y_test, y_pred_thresh)\n",
    "    recall_1 = (y_pred_thresh[y_test == 1] == 1).sum() / (y_test == 1).sum()\n",
    "    precision_1 = (y_test[y_pred_thresh == 1] == 1).sum() / max(1, (y_pred_thresh == 1).sum())\n",
    "    print(f\"Threshold {thresh}: F1={f1:.3f}, Recall={recall_1:.3f}, Precision={precision_1:.3f}\")\n",
    "\n",
    "# Hitung semua recall terlebih dahulu\n",
    "recall_0_plain = (y_pred_plain[y_test == 0] == 0).sum() / (y_test == 0).sum() if (y_test == 0).sum() > 0 else 0\n",
    "recall_1_plain = (y_pred_plain[y_test == 1] == 1).sum() / (y_test == 1).sum() if (y_test == 1).sum() > 0 else 0\n",
    "\n",
    "recall_0_smote = (y_pred_smote[y_test == 0] == 0).sum() / (y_test == 0).sum()\n",
    "recall_1_smote = (y_pred_smote[y_test == 1] == 1).sum() / (y_test == 1).sum()\n",
    "\n",
    "recall_0_ipf = (y_pred_ipf[y_test == 0] == 0).sum() / (y_test == 0).sum()\n",
    "recall_1_ipf = (y_pred_ipf[y_test == 1] == 1).sum() / (y_test == 1).sum()\n",
    "\n",
    "recall_0_bal = (y_pred_balanced[y_test == 0] == 0).sum() / (y_test == 0).sum()\n",
    "recall_1_bal = (y_pred_balanced[y_test == 1] == 1).sum() / (y_test == 1).sum()\n",
    "\n",
    "# Fungsi untuk menentukan status bias\n",
    "def get_bias_status(recall_0, recall_1):\n",
    "    if recall_1 == 0:\n",
    "        return \"Ekstrem\"\n",
    "    diff = abs(recall_0 - recall_1)\n",
    "    if diff > 0.3:\n",
    "        return \"Tinggi\"\n",
    "    elif diff > 0.1:\n",
    "        return \"Sedang\"\n",
    "    else:\n",
    "        return \"Rendah\"\n",
    "\n",
    "bias_plain = get_bias_status(recall_0_plain, recall_1_plain)\n",
    "bias_smote = get_bias_status(recall_0_smote, recall_1_smote)\n",
    "bias_ipf = get_bias_status(recall_0_ipf, recall_1_ipf)\n",
    "bias_bal = get_bias_status(recall_0_bal, recall_1_bal)\n",
    "\n",
    "# Perbandingan akhir LENGKAP\n",
    "print(\"\\n=== PERBANDINGAN BIAS (LENGKAP) ===\")\n",
    "print(\"Method                    | F1-Score | Recall-0 | Recall-1 | Bias\")\n",
    "print(\"-\" * 68)\n",
    "print(f\"Baseline SVM              | {f1_score(y_test, y_pred_plain):.3f}    | {recall_0_plain:.3f}    | {recall_1_plain:.3f}    | {bias_plain}\")\n",
    "print(f\"SVM + SMOTE               | {f1_score(y_test, y_pred_smote):.3f}    | {recall_0_smote:.3f}    | {recall_1_smote:.3f}    | {bias_smote}\")\n",
    "print(f\"SVM + SMOTE-IPF           | {f1_score(y_test, y_pred_ipf):.3f}    | {recall_0_ipf:.3f}    | {recall_1_ipf:.3f}    | {bias_ipf}\")\n",
    "print(f\"SVM + SMOTE-IPF + Balanced| {f1_score(y_test, y_pred_balanced):.3f}    | {recall_0_bal:.3f}    | {recall_1_bal:.3f}    | {bias_bal}\")\n",
    "\n",
    "# Analisis improvement bias\n",
    "print(f\"\\n=== ANALISIS BIAS REDUCTION ===\")\n",
    "print(f\"Baseline: Recall gap = {abs(recall_0_plain - recall_1_plain):.3f} ({bias_plain} bias)\")\n",
    "print(f\"SMOTE: Recall gap = {abs(recall_0_smote - recall_1_smote):.3f} ({bias_smote} bias)\")\n",
    "print(f\"SMOTE-IPF: Recall gap = {abs(recall_0_ipf - recall_1_ipf):.3f} ({bias_ipf} bias)\")\n",
    "print(f\"Balanced: Recall gap = {abs(recall_0_bal - recall_1_bal):.3f} ({bias_bal} bias)\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ KESIMPULAN:\")\n",
    "print(f\"â€¢ Baseline: F1={f1_score(y_test, y_pred_plain):.3f}, Miss {100*(1-recall_1_plain):.1f}% kasus CHD\")\n",
    "print(f\"â€¢ SMOTE: F1={f1_score(y_test, y_pred_smote):.3f}, Miss {100*(1-recall_1_smote):.1f}% kasus CHD\")\n",
    "print(f\"â€¢ SMOTE-IPF: F1={f1_score(y_test, y_pred_ipf):.3f}, Miss {100*(1-recall_1_ipf):.1f}% kasus CHD\")\n",
    "print(f\"â€¢ Balanced: F1={f1_score(y_test, y_pred_balanced):.3f}, Miss {100*(1-recall_1_bal):.1f}% kasus CHD âœ…\")\n",
    "\n",
    "print(f\"\\nðŸ“Š IMPROVEMENT:\")\n",
    "print(f\"â€¢ SMOTE-IPF vs SMOTE: +{(f1_score(y_test, y_pred_ipf) - f1_score(y_test, y_pred_smote)):.3f} F1-score\")\n",
    "print(f\"â€¢ Balanced vs SMOTE-IPF: +{(f1_score(y_test, y_pred_balanced) - f1_score(y_test, y_pred_ipf)):.3f} F1-score\")\n",
    "print(f\"â€¢ Total improvement: +{(f1_score(y_test, y_pred_balanced) - f1_score(y_test, y_pred_plain)):.3f} F1-score\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNGsyVEIzDJLM9tH5/fZbRf",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
