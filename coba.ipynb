{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2932e05c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info Dataset:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4240 entries, 0 to 4239\n",
      "Data columns (total 16 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   male             4240 non-null   int64  \n",
      " 1   age              4240 non-null   int64  \n",
      " 2   education        4135 non-null   float64\n",
      " 3   currentSmoker    4240 non-null   int64  \n",
      " 4   cigsPerDay       4211 non-null   float64\n",
      " 5   BPMeds           4187 non-null   float64\n",
      " 6   prevalentStroke  4240 non-null   int64  \n",
      " 7   prevalentHyp     4240 non-null   int64  \n",
      " 8   diabetes         4240 non-null   int64  \n",
      " 9   totChol          4190 non-null   float64\n",
      " 10  sysBP            4240 non-null   float64\n",
      " 11  diaBP            4240 non-null   float64\n",
      " 12  BMI              4221 non-null   float64\n",
      " 13  heartRate        4239 non-null   float64\n",
      " 14  glucose          3852 non-null   float64\n",
      " 15  TenYearCHD       4240 non-null   int64  \n",
      "dtypes: float64(9), int64(7)\n",
      "memory usage: 530.1 KB\n",
      "None\n",
      "\n",
      "Distribusi Kelas Target (TenYearCHD):\n",
      "TenYearCHD\n",
      "0    3596\n",
      "1     644\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Jumlah Data Latih: 3392\n",
      "Jumlah Data Uji: 848\n",
      "\n",
      "==================== Skenario 1: Baseline (Tanpa Oversampling) ====================\n",
      "Confusion Matrix:\n",
      "[[717   2]\n",
      " [128   1]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.85      1.00      0.92       719\n",
      "         1.0       0.33      0.01      0.02       129\n",
      "\n",
      "    accuracy                           0.85       848\n",
      "   macro avg       0.59      0.50      0.47       848\n",
      "weighted avg       0.77      0.85      0.78       848\n",
      "\n",
      "Accuracy: 0.8467\n",
      "\n",
      "[Info] Data Latih setelah SMOTE: (5754, 15)\n",
      "\n",
      "==================== Skenario 2: SMOTE ====================\n",
      "Confusion Matrix:\n",
      "[[498 221]\n",
      " [ 57  72]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.69      0.78       719\n",
      "         1.0       0.25      0.56      0.34       129\n",
      "\n",
      "    accuracy                           0.67       848\n",
      "   macro avg       0.57      0.63      0.56       848\n",
      "weighted avg       0.80      0.67      0.71       848\n",
      "\n",
      "Accuracy: 0.6722\n",
      "\n",
      "[Info] Data Latih setelah SMOTE-Filtering: (4156, 15)\n",
      "\n",
      "==================== Skenario 3: SMOTE + Filtering (IPF Approach) ====================\n",
      "Confusion Matrix:\n",
      "[[367 352]\n",
      " [ 34  95]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.51      0.66       719\n",
      "         1.0       0.21      0.74      0.33       129\n",
      "\n",
      "    accuracy                           0.54       848\n",
      "   macro avg       0.56      0.62      0.49       848\n",
      "weighted avg       0.81      0.54      0.61       848\n",
      "\n",
      "Accuracy: 0.5448\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTEENN \n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3.1 & 3.3.1 Pengumpulan dan Explorasi Data Awal\n",
    "# ---------------------------------------------------------\n",
    "# Pastikan file 'framingham.csv' ada di direktori yang sama\n",
    "df = pd.read_csv('framingham.csv')\n",
    "\n",
    "print(\"Info Dataset:\")\n",
    "print(df.info())\n",
    "print(\"\\nDistribusi Kelas Target (TenYearCHD):\")\n",
    "print(df['TenYearCHD'].value_counts())\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3.3.4 Pre-Processing Data\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# 1. Penanganan Missing Values (Imputasi Mean)\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "\n",
    "# 2. Normalisasi (Min-Max Scaling)\n",
    "# Sesuai teks: dilakukan sebelum split\n",
    "X = df_imputed.drop('TenYearCHD', axis=1)\n",
    "y = df_imputed['TenYearCHD']\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3.3.5 Split Data\n",
    "# ---------------------------------------------------------\n",
    "# Proporsi 80:20, Stratified\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nJumlah Data Latih: {X_train.shape[0]}\")\n",
    "print(f\"Jumlah Data Uji: {X_test.shape[0]}\")\n",
    "\n",
    "# Fungsi untuk melatih dan evaluasi model\n",
    "def evaluate_model(X_tr, y_tr, X_te, y_te, scenario_name):\n",
    "    print(f\"\\n{'='*20} {scenario_name} {'='*20}\")\n",
    "    \n",
    "    # Inisialisasi SVM\n",
    "    svm_model = SVC(kernel='rbf', random_state=42)\n",
    "    \n",
    "    # Pelatihan\n",
    "    svm_model.fit(X_tr, y_tr)\n",
    "    \n",
    "    # Prediksi\n",
    "    y_pred = svm_model.predict(X_te)\n",
    "    \n",
    "    # Evaluasi\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_te, y_pred))\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_te, y_pred))\n",
    "    print(f\"Accuracy: {accuracy_score(y_te, y_pred):.4f}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3.3.7 Evaluasi Model (3 Skenario)\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# Skenario 1: Model Baseline (Tanpa Oversampling)\n",
    "evaluate_model(X_train, y_train, X_test, y_test, \"Skenario 1: Baseline (Tanpa Oversampling)\")\n",
    "\n",
    "# Skenario 2: Model SMOTE\n",
    "# Diterapkan hanya pada Data Latih (Post-split)\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(f\"\\n[Info] Data Latih setelah SMOTE: {X_train_smote.shape}\")\n",
    "evaluate_model(X_train_smote, y_train_smote, X_test, y_test, \"Skenario 2: SMOTE\")\n",
    "\n",
    "# Skenario 3: Model SMOTE-IPF (Diwakili oleh SMOTE + Cleaning)\n",
    "# Menggunakan SMOTEENN untuk simulasi SMOTE + Filtering outlier\n",
    "smote_ipf = SMOTEENN(random_state=42)\n",
    "X_train_ipf, y_train_ipf = smote_ipf.fit_resample(X_train, y_train)\n",
    "\n",
    "print(f\"\\n[Info] Data Latih setelah SMOTE-Filtering: {X_train_ipf.shape}\")\n",
    "evaluate_model(X_train_ipf, y_train_ipf, X_test, y_test, \"Skenario 3: SMOTE + Filtering (IPF Approach)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c9b8713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensi Data Baru: (4240, 5)\n",
      "Fitur yang digunakan: ['age', 'sysBP', 'cigsPerDay', 'totChol']\n",
      "\n",
      ">>> Skenario 1: 4 Fitur - Tanpa SMOTE\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.63      0.74       719\n",
      "         1.0       0.23      0.62      0.34       129\n",
      "\n",
      "    accuracy                           0.63       848\n",
      "   macro avg       0.57      0.63      0.54       848\n",
      "weighted avg       0.80      0.63      0.68       848\n",
      "\n",
      "Confusion Matrix:\n",
      " [[453 266]\n",
      " [ 49  80]]\n",
      "\n",
      ">>> Skenario 2: 4 Fitur - SMOTE\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.62      0.74       719\n",
      "         1.0       0.23      0.63      0.34       129\n",
      "\n",
      "    accuracy                           0.62       848\n",
      "   macro avg       0.57      0.62      0.54       848\n",
      "weighted avg       0.80      0.62      0.67       848\n",
      "\n",
      "Confusion Matrix:\n",
      " [[446 273]\n",
      " [ 48  81]]\n",
      "\n",
      ">>> Skenario 3: 4 Fitur - SMOTE-IPF\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.60      0.72       719\n",
      "         1.0       0.22      0.63      0.33       129\n",
      "\n",
      "    accuracy                           0.61       848\n",
      "   macro avg       0.56      0.62      0.52       848\n",
      "weighted avg       0.80      0.61      0.66       848\n",
      "\n",
      "Confusion Matrix:\n",
      " [[433 286]\n",
      " [ 48  81]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---------------------------------------------------------\n",
    "# FILTERING FITUR (Hanya 4 Fitur Utama + Target)\n",
    "# ---------------------------------------------------------\n",
    "# Kita memilih fitur yang umumnya memiliki korelasi tertinggi\n",
    "selected_features = ['age', 'sysBP', 'cigsPerDay', 'totChol', 'TenYearCHD']\n",
    "df_selected = df[selected_features]\n",
    "\n",
    "print(f\"Dimensi Data Baru: {df_selected.shape}\")\n",
    "print(\"Fitur yang digunakan:\", list(df_selected.columns[:-1]))\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Pre-Processing\n",
    "# ---------------------------------------------------------\n",
    "# 1. Imputasi Mean\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "df_imputed = pd.DataFrame(imputer.fit_transform(df_selected), columns=selected_features)\n",
    "\n",
    "# 2. Normalisasi MinMax\n",
    "X = df_imputed.drop('TenYearCHD', axis=1)\n",
    "y = df_imputed['TenYearCHD']\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "\n",
    "# 3. Split Data (80:20 Stratified)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Fungsi Evaluasi\n",
    "# ---------------------------------------------------------\n",
    "def run_experiment(X_tr, y_tr, X_te, y_te, scenario):\n",
    "    print(f\"\\n>>> {scenario}\")\n",
    "    # Menggunakan class_weight='balanced' untuk membantu menaikkan sensitivitas ke kelas minoritas\n",
    "    model = SVC(kernel='rbf', C=1.0, random_state=42, class_weight='balanced') \n",
    "    model.fit(X_tr, y_tr)\n",
    "    y_pred = model.predict(X_te)\n",
    "    \n",
    "    print(classification_report(y_te, y_pred))\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_te, y_pred))\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1. Tanpa Oversampling (4 Fitur)\n",
    "# ---------------------------------------------------------\n",
    "run_experiment(X_train, y_train, X_test, y_test, \"Skenario 1: 4 Fitur - Tanpa SMOTE\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. SMOTE (4 Fitur)\n",
    "# ---------------------------------------------------------\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "run_experiment(X_train_smote, y_train_smote, X_test, y_test, \"Skenario 2: 4 Fitur - SMOTE\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. SMOTE-IPF / SMOTEENN (4 Fitur)\n",
    "# ---------------------------------------------------------\n",
    "# SMOTEENN digunakan sebagai implementasi teknis untuk SMOTE + Cleaning\n",
    "smote_ipf = SMOTEENN(random_state=42)\n",
    "X_train_ipf, y_train_ipf = smote_ipf.fit_resample(X_train, y_train)\n",
    "\n",
    "run_experiment(X_train_ipf, y_train_ipf, X_test, y_test, \"Skenario 3: 4 Fitur - SMOTE-IPF\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
